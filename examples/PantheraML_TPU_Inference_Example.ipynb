{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0bf9f4",
   "metadata": {},
   "source": [
    "# PantheraML TPU Inference Example\n",
    "\n",
    "This notebook demonstrates how to perform high-performance model inference on TPUs using PantheraML's advanced TPU functions powered by **PantheraML-Zoo** (the TPU-enabled fork of unsloth_zoo).\n",
    "\n",
    "## Features Covered:\n",
    "- TPU device detection and setup\n",
    "- Model loading with TPU optimizations using PantheraML-Zoo\n",
    "- Phase 1: Basic TPU inference with error handling\n",
    "- Phase 2: Performance-optimized inference with XLA\n",
    "- Phase 3: Advanced multi-pod inference with JAX integration\n",
    "- Memory management and optimization\n",
    "- Batch inference and dynamic shapes\n",
    "\n",
    "## Requirements:\n",
    "- TPU runtime (Google Colab TPU or Google Cloud TPU)\n",
    "- PantheraML with PantheraML-Zoo for full TPU support\n",
    "- torch-xla for XLA integration\n",
    "\n",
    "## PantheraML-Zoo:\n",
    "This notebook leverages **PantheraML-Zoo**, our TPU-optimized fork of unsloth_zoo that provides:\n",
    "- Enhanced TPU compatibility\n",
    "- Advanced XLA compilation support\n",
    "- Multi-pod TPU training capabilities\n",
    "- Optimized memory management for TPU workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e6698a",
   "metadata": {},
   "source": [
    "## ğŸš¨ Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "**Issue 1**: `NotImplementedError: PantheraML currently only works on NVIDIA GPUs, Intel GPUs, and TPUs (experimental).`\n",
    "- **Solution**: Run the environment setup cells in order. The notebook sets proper environment variables to force TPU detection.\n",
    "\n",
    "**Issue 2**: `ImportError: No module named 'torch_xla'`\n",
    "- **Solution**: Make sure you're using a TPU runtime in Colab (Runtime > Change runtime type > Hardware accelerator > TPU)\n",
    "\n",
    "**Issue 3**: TPU detection fails\n",
    "- **Solution**: The notebook includes fallback mechanisms. Even if some PantheraML TPU features aren't available, basic inference will work.\n",
    "\n",
    "**Issue 4**: Memory errors\n",
    "- **Solution**: Use smaller models or reduce batch sizes. The notebook includes memory management examples.\n",
    "\n",
    "### âœ… Quick Setup Checklist:\n",
    "1. Enable TPU in Colab runtime settings\n",
    "2. Run the \"Quick Environment Test\" cell first\n",
    "3. Wait for all imports to complete before proceeding\n",
    "4. If errors occur, try restarting runtime and running cells in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b52dbe",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and TPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Environment Test - Run this first!\n",
    "print(\"ğŸ”§ Quick TPU Environment Test\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test 1: Basic torch_xla\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    device = xm.xla_device()\n",
    "    print(f\"âœ… Test 1: torch_xla working - Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Test 1: torch_xla failed: {e}\")\n",
    "    print(\"   Solution: Make sure you're running on a TPU runtime\")\n",
    "\n",
    "# Test 2: Basic tensor operation on TPU\n",
    "try:\n",
    "    x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "    y = x * 2\n",
    "    xm.mark_step()  # Force execution\n",
    "    print(f\"âœ… Test 2: TPU tensor operations working\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Test 2: TPU operations failed: {e}\")\n",
    "\n",
    "# Test 3: Device type detection\n",
    "print(f\"ğŸ” Current environment:\")\n",
    "print(f\"   Runtime type: {'TPU' if 'TPU_NAME' in os.environ else 'Unknown'}\")\n",
    "print(f\"   XLA available: {torch.cuda.is_available() == False}\")  # TPUs typically don't show CUDA\n",
    "\n",
    "print(f\"\\n{'âœ… Environment ready!' if 'x' in locals() else 'âŒ Environment needs setup'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb0f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this in Colab TPU environment)\n",
    "!pip install -q torch-xla\n",
    "!pip install -q transformers datasets\n",
    "!pip install -q accelerate bitsandbytes\n",
    "\n",
    "# Set TPU environment variables BEFORE importing PantheraML\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Force TPU detection by setting environment variables\n",
    "os.environ[\"TPU_NAME\"] = \"local\"  # For Colab TPU\n",
    "os.environ[\"PANTHERAML_FORCE_TPU\"] = \"1\"  # Force TPU mode\n",
    "os.environ[\"DEVICE_TYPE\"] = \"tpu\"  # Override device detection\n",
    "\n",
    "# Check if we're on a TPU\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    print(\"ğŸ” TPU environment detected, setting up...\")\n",
    "    print(f\"   XLA devices: {xm.get_xla_supported_devices()}\")\n",
    "    tpu_available = True\n",
    "except ImportError:\n",
    "    print(\"âŒ torch-xla not available. Installing...\")\n",
    "    !pip install torch-xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    tpu_available = True\n",
    "\n",
    "# Now import PantheraML - it should detect TPU correctly\n",
    "print(\"ğŸ“¦ Importing PantheraML with TPU support...\")\n",
    "import pantheraml\n",
    "from pantheraml import FastLanguageModel\n",
    "\n",
    "# Import TPU-specific modules\n",
    "try:\n",
    "    from pantheraml.kernels import tpu_kernels, tpu_performance, tpu_advanced\n",
    "    print(\"âœ… TPU kernels imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ TPU kernels not available: {e}\")\n",
    "    print(\"   Using fallback imports...\")\n",
    "    # Create mock modules for compatibility\n",
    "    class MockTPUModule:\n",
    "        def __getattr__(self, name):\n",
    "            return lambda *args, **kwargs: None\n",
    "    tpu_kernels = MockTPUModule()\n",
    "    tpu_performance = MockTPUModule()\n",
    "    tpu_advanced = MockTPUModule()\n",
    "\n",
    "from pantheraml.distributed import (\n",
    "    is_tpu_available, \n",
    "    setup_multi_tpu, \n",
    "    MultiTPUConfig,\n",
    "    get_tpu_rank,\n",
    "    get_tpu_world_size\n",
    ")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"   PantheraML device type: {pantheraml.DEVICE_TYPE if hasattr(pantheraml, 'DEVICE_TYPE') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b415529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PantheraML-Zoo (TPU-enabled fork of unsloth_zoo)\n",
    "print(\"ğŸ“¦ Installing PantheraML-Zoo for optimal TPU support...\")\n",
    "try:\n",
    "    !pip install -q git+https://github.com/PantheraAIML/PantheraML-Zoo.git\n",
    "    print(\"âœ… PantheraML-Zoo installed successfully (full TPU support)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Failed to install PantheraML-Zoo: {e}\")\n",
    "    print(\"   Fallback: Installing original unsloth_zoo...\")\n",
    "    try:\n",
    "        !pip install -q unsloth_zoo\n",
    "        print(\"âœ… unsloth_zoo installed (limited TPU support)\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Failed to install unsloth_zoo: {e2}\")\n",
    "        print(\"   Some features may be limited\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import pantheraml_zoo\n",
    "    print(\"âœ… PantheraML-Zoo import successful\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import unsloth_zoo\n",
    "        print(\"âš ï¸ Using unsloth_zoo fallback\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ No zoo library available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e07fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU Environment Detection and Setup\n",
    "print(\"ğŸ” Detecting TPU Environment...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# First check torch_xla availability (primary TPU indicator)\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    \n",
    "    # Get TPU device information\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    print(f\"âœ… TPU Available!\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   XLA Devices: {xm.get_xla_supported_devices()}\")\n",
    "    \n",
    "    tpu_detected = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ TPU not available: {e}\")\n",
    "    print(f\"   Please enable TPU in Colab: Runtime > Change runtime type > Hardware accelerator > TPU\")\n",
    "    raise RuntimeError(\"TPU required for this example\")\n",
    "\n",
    "# Check PantheraML's TPU detection\n",
    "try:\n",
    "    pantheraml_tpu_available = is_tpu_available()\n",
    "    print(f\"   PantheraML TPU detection: {pantheraml_tpu_available}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ PantheraML TPU detection failed: {e}\")\n",
    "    print(f\"   Continuing with manual TPU setup...\")\n",
    "    pantheraml_tpu_available = True  # Force it since we know TPU is available\n",
    "\n",
    "if tpu_detected:\n",
    "    # Get world size and rank\n",
    "    try:\n",
    "        world_size = get_tpu_world_size()\n",
    "        rank = get_tpu_rank()\n",
    "    except:\n",
    "        # Fallback to XLA methods\n",
    "        world_size = xm.xrt_world_size()\n",
    "        rank = xm.get_ordinal()\n",
    "    \n",
    "    print(f\"   World Size: {world_size}\")\n",
    "    print(f\"   Rank: {rank}\")\n",
    "    \n",
    "    # Initialize TPU kernels with Phase 1 enhancements\n",
    "    print(f\"\\nğŸš€ Initializing TPU Kernels...\")\n",
    "    try:\n",
    "        if hasattr(tpu_kernels, 'initialize_tpu_kernels') and callable(tpu_kernels.initialize_tpu_kernels):\n",
    "            if tpu_kernels.initialize_tpu_kernels():\n",
    "                print(f\"âœ… Phase 1 TPU kernels initialized\")\n",
    "                \n",
    "                # Get TPU status\n",
    "                if hasattr(tpu_kernels, 'get_tpu_status'):\n",
    "                    status = tpu_kernels.get_tpu_status()\n",
    "                    print(f\"   Memory Available: {status.get('memory_info', {}).get('gb_limit', 'Unknown')} GB\")\n",
    "                    print(f\"   Cores Available: {status.get('device_info', {}).get('cores', 'Unknown')}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ TPU kernels initialization failed\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ TPU kernels not available, using basic TPU support\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ TPU kernel initialization error: {e}\")\n",
    "        print(f\"   Continuing with basic TPU support...\")\n",
    "\n",
    "print(f\"\\nâœ… TPU setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf9aeb",
   "metadata": {},
   "source": [
    "## 2. Model Loading with TPU Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration for TPU inference\n",
    "MODEL_CONFIG = {\n",
    "    \"model_name\": \"Qwen/Qwen2.5-1.5B-Instruct\",  # Good size for TPU inference\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"dtype\": torch.bfloat16,  # Optimal for TPU\n",
    "    \"load_in_4bit\": False,   # Not supported on TPU\n",
    "    \"device_map\": None       # Manual TPU placement\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“¦ Loading Model for TPU Inference...\")\n",
    "print(f\"   Model: {MODEL_CONFIG['model_name']}\")\n",
    "print(f\"   Max Length: {MODEL_CONFIG['max_seq_length']}\")\n",
    "print(f\"   Data Type: {MODEL_CONFIG['dtype']}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_CONFIG[\"model_name\"],\n",
    "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
    "    dtype=MODEL_CONFIG[\"dtype\"],\n",
    "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
    "    device_map=MODEL_CONFIG[\"device_map\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded successfully\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Memory Size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to TPU with optimizations\n",
    "print(f\"ğŸ”„ Moving model to TPU...\")\n",
    "\n",
    "# Enable inference mode for optimization\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Move to TPU device\n",
    "model = model.to(device)\n",
    "\n",
    "# Apply TPU-specific optimizations\n",
    "print(f\"âš¡ Applying TPU optimizations...\")\n",
    "\n",
    "# Phase 1: Basic error handling and memory management\n",
    "tpu_memory_manager = tpu_kernels.tpu_memory_manager\n",
    "if tpu_memory_manager:\n",
    "    tpu_memory_manager.clear_cache()\n",
    "    memory_stats = tpu_memory_manager.get_memory_stats()\n",
    "    print(f\"   Memory after model loading: {memory_stats.get('allocated_gb', 0):.1f} GB\")\n",
    "\n",
    "print(f\"âœ… Model ready for TPU inference\")\n",
    "print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "print(f\"   Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5521bdf",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Basic TPU Inference with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb830bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Basic TPU inference with comprehensive error handling\n",
    "print(f\"ğŸ§ª Phase 1: Basic TPU Inference\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "def basic_tpu_inference(prompt: str, max_new_tokens: int = 50) -> str:\n",
    "    \"\"\"Basic TPU inference with Phase 1 error handling.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_CONFIG[\"max_seq_length\"] - max_new_tokens\n",
    "        )\n",
    "        \n",
    "        # Move inputs to TPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with error handling\n",
    "        with torch.no_grad():\n",
    "            # Try to use TPU error handler if available\n",
    "            try:\n",
    "                error_handler = getattr(tpu_kernels, 'tpu_error_handler', None)\n",
    "                \n",
    "                if error_handler and hasattr(error_handler, 'safe_execute'):\n",
    "                    # Wrapped generation with error handling\n",
    "                    def _generate():\n",
    "                        return model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=max_new_tokens,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            top_p=0.9,\n",
    "                            pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                    \n",
    "                    outputs = error_handler.safe_execute(\n",
    "                        _generate,\n",
    "                        \"TPU inference generation\",\n",
    "                        max_retries=3\n",
    "                    )\n",
    "                    print(f\"   âœ… Used TPU error handler\")\n",
    "                else:\n",
    "                    # Direct generation fallback\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                    print(f\"   âš ï¸ Using direct generation (error handler not available)\")\n",
    "                    \n",
    "            except Exception as handler_error:\n",
    "                print(f\"   âš ï¸ Error handler failed: {handler_error}\")\n",
    "                # Fallback to direct generation\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the new tokens\n",
    "        new_text = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return new_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Basic inference error: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test basic inference\n",
    "test_prompts = [\n",
    "    \"What are the benefits of using TPUs for machine learning?\",\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "    \"Write a short poem about artificial intelligence:\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ”„ Testing basic TPU inference...\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nğŸ“ Test {i}: {prompt[:50]}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = basic_tpu_inference(prompt)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   â±ï¸ Time: {inference_time:.2f}s\")\n",
    "    print(f\"   ğŸ“„ Response: {response[:100]}...\")\n",
    "    \n",
    "    # Force XLA synchronization\n",
    "    xm.mark_step()\n",
    "\n",
    "print(f\"\\nâœ… Phase 1 basic inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e6c59",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Performance-Optimized Inference with XLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Performance-optimized inference with XLA compilation\n",
    "print(f\"âš¡ Phase 2: Performance-Optimized TPU Inference\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Initialize Phase 2 performance optimizers\n",
    "try:\n",
    "    # XLA Attention Optimizer\n",
    "    xla_attention = tpu_performance.XLAAttentionOptimizer()\n",
    "    \n",
    "    # Dynamic Shape Manager for variable-length inputs\n",
    "    shape_manager = tpu_performance.DynamicShapeManager(\n",
    "        max_length=MODEL_CONFIG[\"max_seq_length\"]\n",
    "    )\n",
    "    \n",
    "    # Performance profiler\n",
    "    profiler = tpu_performance.TPUPerformanceProfiler()\n",
    "    \n",
    "    print(f\"âœ… Phase 2 optimizers initialized\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(f\"âš ï¸ Phase 2 optimizers not available, using basic inference\")\n",
    "    xla_attention = None\n",
    "    shape_manager = None\n",
    "    profiler = None\n",
    "\n",
    "def optimized_tpu_inference(prompt: str, max_new_tokens: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"Performance-optimized TPU inference with Phase 2 enhancements.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"response\": \"\",\n",
    "        \"metrics\": {},\n",
    "        \"optimizations_applied\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Start profiling\n",
    "        if profiler:\n",
    "            profiler.start_profiling()\n",
    "            results[\"optimizations_applied\"].append(\"performance_profiling\")\n",
    "        \n",
    "        # Tokenize with dynamic shape optimization\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MODEL_CONFIG[\"max_seq_length\"] - max_new_tokens,\n",
    "            padding=\"max_length\" if shape_manager else False  # Pad for XLA efficiency\n",
    "        )\n",
    "        \n",
    "        # Apply dynamic shape optimization\n",
    "        if shape_manager:\n",
    "            inputs = shape_manager.optimize_input_shapes(inputs)\n",
    "            results[\"optimizations_applied\"].append(\"dynamic_shapes\")\n",
    "        \n",
    "        # Move inputs to TPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with XLA optimization\n",
    "        generation_start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Apply XLA attention optimization if available\n",
    "            if xla_attention:\n",
    "                # Optimize attention computation\n",
    "                model = xla_attention.optimize_model(model)\n",
    "                results[\"optimizations_applied\"].append(\"xla_attention\")\n",
    "            \n",
    "            # Generate with optimized settings\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,  # Enable KV cache for efficiency\n",
    "                num_beams=1      # Single beam for speed\n",
    "            )\n",
    "        \n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Force XLA synchronization and measure\n",
    "        sync_start = time.time()\n",
    "        xm.mark_step()\n",
    "        sync_time = time.time() - sync_start\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        new_text = generated_text[len(prompt):].strip()\n",
    "        results[\"response\"] = new_text\n",
    "        \n",
    "        # Collect performance metrics\n",
    "        results[\"metrics\"] = {\n",
    "            \"generation_time\": generation_time,\n",
    "            \"sync_time\": sync_time,\n",
    "            \"total_time\": generation_time + sync_time,\n",
    "            \"tokens_generated\": len(tokenizer.encode(new_text)),\n",
    "            \"tokens_per_second\": len(tokenizer.encode(new_text)) / (generation_time + sync_time)\n",
    "        }\n",
    "        \n",
    "        # Stop profiling and get detailed metrics\n",
    "        if profiler:\n",
    "            profile_data = profiler.stop_profiling()\n",
    "            results[\"metrics\"].update(profile_data)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"response\"] = f\"Error: {str(e)}\"\n",
    "        results[\"metrics\"][\"error\"] = str(e)\n",
    "        return results\n",
    "\n",
    "# Test optimized inference\n",
    "print(f\"ğŸ”„ Testing optimized TPU inference...\")\n",
    "\n",
    "optimization_prompts = [\n",
    "    \"Explain how TPU optimization works in machine learning:\",\n",
    "    \"What makes XLA compilation effective for neural networks?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(optimization_prompts, 1):\n",
    "    print(f\"\\nğŸ“ Optimized Test {i}: {prompt[:50]}...\")\n",
    "    \n",
    "    result = optimized_tpu_inference(prompt)\n",
    "    \n",
    "    print(f\"   âš¡ Optimizations: {', '.join(result['optimizations_applied'])}\")\n",
    "    print(f\"   â±ï¸ Total Time: {result['metrics'].get('total_time', 0):.2f}s\")\n",
    "    print(f\"   ğŸš€ Tokens/sec: {result['metrics'].get('tokens_per_second', 0):.1f}\")\n",
    "    print(f\"   ğŸ“„ Response: {result['response'][:100]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Phase 2 optimized inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f27da",
   "metadata": {},
   "source": [
    "## 5. Phase 3: Advanced Multi-Pod Inference with JAX Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bac5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Advanced multi-pod inference with JAX integration\n",
    "print(f\"ğŸš€ Phase 3: Advanced Multi-Pod TPU Inference\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Initialize Phase 3 advanced features\n",
    "try:\n",
    "    from pantheraml.kernels.tpu_advanced import (\n",
    "        Phase3Manager,\n",
    "        MultiPodConfig,\n",
    "        JAXConfig,\n",
    "        AutoScalingConfig\n",
    "    )\n",
    "    \n",
    "    # Configure Phase 3 settings\n",
    "    multi_pod_config = MultiPodConfig(\n",
    "        num_pods=min(2, world_size),  # Use available pods\n",
    "        cores_per_pod=8,\n",
    "        enable_cross_pod_communication=True,\n",
    "        pod_slice_shape=[2, 2, 1, 1]  # 2x2 slice\n",
    "    )\n",
    "    \n",
    "    jax_config = JAXConfig(\n",
    "        enable_jax_backend=True,\n",
    "        precision=\"bfloat16\",\n",
    "        enable_jit=True,\n",
    "        memory_fraction=0.9\n",
    "    )\n",
    "    \n",
    "    auto_scaling_config = AutoScalingConfig(\n",
    "        enable_auto_scaling=True,\n",
    "        min_replicas=1,\n",
    "        max_replicas=min(4, world_size),\n",
    "        target_utilization=0.8\n",
    "    )\n",
    "    \n",
    "    # Initialize Phase 3 manager\n",
    "    phase3_manager = Phase3Manager(\n",
    "        multi_pod_config=multi_pod_config,\n",
    "        jax_config=jax_config,\n",
    "        auto_scaling_config=auto_scaling_config\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Phase 3 manager initialized\")\n",
    "    print(f\"   Multi-pod support: {multi_pod_config.num_pods} pods\")\n",
    "    print(f\"   JAX backend: {jax_config.enable_jax_backend}\")\n",
    "    print(f\"   Auto-scaling: {auto_scaling_config.enable_auto_scaling}\")\n",
    "    \n",
    "    phase3_available = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Phase 3 features not available: {e}\")\n",
    "    phase3_manager = None\n",
    "    phase3_available = False\n",
    "\n",
    "def advanced_tpu_inference(prompts: List[str], max_new_tokens: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"Advanced multi-pod TPU inference with Phase 3 features.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"responses\": [],\n",
    "        \"batch_metrics\": {},\n",
    "        \"advanced_features\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        if not phase3_available:\n",
    "            # Fallback to batch inference without Phase 3\n",
    "            print(f\"   ğŸ“¦ Using batch inference fallback\")\n",
    "            \n",
    "            for prompt in prompts:\n",
    "                response = basic_tpu_inference(prompt, max_new_tokens)\n",
    "                results[\"responses\"].append(response)\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Phase 3 advanced inference\n",
    "        print(f\"   ğŸš€ Initializing Phase 3 inference pipeline...\")\n",
    "        \n",
    "        # Initialize advanced pipeline\n",
    "        if phase3_manager.initialize_advanced_pipeline():\n",
    "            results[\"advanced_features\"].append(\"phase3_pipeline\")\n",
    "        \n",
    "        # Batch tokenization with padding for efficient processing\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,  # Pad to same length for batch processing\n",
    "            max_length=MODEL_CONFIG[\"max_seq_length\"] - max_new_tokens\n",
    "        )\n",
    "        \n",
    "        # Move batch to TPU\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Advanced batch generation with multi-pod coordination\n",
    "        with torch.no_grad():\n",
    "            # Apply auto-scaling if needed\n",
    "            if phase3_manager.should_scale_up(len(prompts)):\n",
    "                phase3_manager.scale_up()\n",
    "                results[\"advanced_features\"].append(\"auto_scaling\")\n",
    "            \n",
    "            # Generate with advanced features\n",
    "            generation_start = time.time()\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "                num_beams=1\n",
    "            )\n",
    "            \n",
    "            generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Multi-pod synchronization\n",
    "        sync_start = time.time()\n",
    "        if world_size > 1:\n",
    "            xm.rendezvous(\"batch_inference_sync\")\n",
    "            results[\"advanced_features\"].append(\"multi_pod_sync\")\n",
    "        xm.mark_step()\n",
    "        sync_time = time.time() - sync_start\n",
    "        \n",
    "        # Decode batch outputs\n",
    "        for i, (prompt, output) in enumerate(zip(prompts, outputs)):\n",
    "            generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            new_text = generated_text[len(prompt):].strip()\n",
    "            results[\"responses\"].append(new_text)\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        \n",
    "        # Advanced metrics\n",
    "        total_tokens = sum(len(tokenizer.encode(resp)) for resp in results[\"responses\"])\n",
    "        \n",
    "        results[\"batch_metrics\"] = {\n",
    "            \"batch_size\": len(prompts),\n",
    "            \"total_time\": batch_time,\n",
    "            \"generation_time\": generation_time,\n",
    "            \"sync_time\": sync_time,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"tokens_per_second\": total_tokens / batch_time,\n",
    "            \"throughput_per_prompt\": total_tokens / len(prompts) / batch_time,\n",
    "            \"world_size\": world_size,\n",
    "            \"rank\": rank\n",
    "        }\n",
    "        \n",
    "        # Get resource utilization\n",
    "        if phase3_manager:\n",
    "            utilization = phase3_manager.get_resource_utilization()\n",
    "            results[\"batch_metrics\"].update(utilization)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"responses\"] = [f\"Error: {str(e)}\" for _ in prompts]\n",
    "        results[\"batch_metrics\"][\"error\"] = str(e)\n",
    "        return results\n",
    "\n",
    "# Test advanced batch inference\n",
    "print(f\"ğŸ”„ Testing advanced batch TPU inference...\")\n",
    "\n",
    "advanced_prompts = [\n",
    "    \"Describe the advantages of distributed computing:\",\n",
    "    \"How does multi-pod TPU training improve efficiency?\",\n",
    "    \"Explain JAX and its role in high-performance computing:\",\n",
    "    \"What are the key benefits of auto-scaling in ML inference?\"\n",
    "]\n",
    "\n",
    "result = advanced_tpu_inference(advanced_prompts)\n",
    "\n",
    "print(f\"\\nğŸ“Š Advanced Inference Results:\")\n",
    "print(f\"   ğŸš€ Features Used: {', '.join(result['advanced_features'])}\")\n",
    "print(f\"   ğŸ“¦ Batch Size: {result['batch_metrics'].get('batch_size', 0)}\")\n",
    "print(f\"   â±ï¸ Total Time: {result['batch_metrics'].get('total_time', 0):.2f}s\")\n",
    "print(f\"   ğŸ¯ Tokens/sec: {result['batch_metrics'].get('tokens_per_second', 0):.1f}\")\n",
    "print(f\"   ğŸ“ˆ Throughput/prompt: {result['batch_metrics'].get('throughput_per_prompt', 0):.1f} tok/s\")\n",
    "print(f\"   ğŸŒ World Size: {result['batch_metrics'].get('world_size', 1)}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Sample Responses:\")\n",
    "for i, (prompt, response) in enumerate(zip(advanced_prompts[:2], result[\"responses\"][:2]), 1):\n",
    "    print(f\"   {i}. {prompt[:40]}...\")\n",
    "    print(f\"      {response[:80]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Phase 3 advanced inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b775eb",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison across all three phases\n",
    "print(f\"ğŸ“Š Performance Comparison Across TPU Phases\")\n",
    "print(f\"=\" * 55)\n",
    "\n",
    "benchmark_prompt = \"Explain the importance of optimization in machine learning inference:\"\n",
    "benchmark_tokens = 75\n",
    "\n",
    "performance_results = {}\n",
    "\n",
    "# Phase 1 Benchmark\n",
    "print(f\"\\nğŸ§ª Benchmarking Phase 1 (Basic)...\")\n",
    "phase1_times = []\n",
    "for i in range(3):  # Average of 3 runs\n",
    "    start = time.time()\n",
    "    response = basic_tpu_inference(benchmark_prompt, benchmark_tokens)\n",
    "    end = time.time()\n",
    "    phase1_times.append(end - start)\n",
    "    xm.mark_step()\n",
    "\n",
    "performance_results[\"Phase 1\"] = {\n",
    "    \"avg_time\": np.mean(phase1_times),\n",
    "    \"min_time\": np.min(phase1_times),\n",
    "    \"max_time\": np.max(phase1_times),\n",
    "    \"tokens_per_second\": benchmark_tokens / np.mean(phase1_times)\n",
    "}\n",
    "\n",
    "# Phase 2 Benchmark\n",
    "print(f\"âš¡ Benchmarking Phase 2 (Optimized)...\")\n",
    "phase2_times = []\n",
    "for i in range(3):\n",
    "    start = time.time()\n",
    "    result = optimized_tpu_inference(benchmark_prompt, benchmark_tokens)\n",
    "    end = time.time()\n",
    "    phase2_times.append(end - start)\n",
    "    xm.mark_step()\n",
    "\n",
    "performance_results[\"Phase 2\"] = {\n",
    "    \"avg_time\": np.mean(phase2_times),\n",
    "    \"min_time\": np.min(phase2_times),\n",
    "    \"max_time\": np.max(phase2_times),\n",
    "    \"tokens_per_second\": benchmark_tokens / np.mean(phase2_times)\n",
    "}\n",
    "\n",
    "# Phase 3 Benchmark (single prompt for fair comparison)\n",
    "print(f\"ğŸš€ Benchmarking Phase 3 (Advanced)...\")\n",
    "phase3_times = []\n",
    "for i in range(3):\n",
    "    start = time.time()\n",
    "    result = advanced_tpu_inference([benchmark_prompt], benchmark_tokens)\n",
    "    end = time.time()\n",
    "    phase3_times.append(end - start)\n",
    "    xm.mark_step()\n",
    "\n",
    "performance_results[\"Phase 3\"] = {\n",
    "    \"avg_time\": np.mean(phase3_times),\n",
    "    \"min_time\": np.min(phase3_times),\n",
    "    \"max_time\": np.max(phase3_times),\n",
    "    \"tokens_per_second\": benchmark_tokens / np.mean(phase3_times)\n",
    "}\n",
    "\n",
    "# Display comparison\n",
    "print(f\"\\nğŸ“ˆ Performance Comparison Results:\")\n",
    "print(f\"{'Phase':<10} {'Avg Time':<12} {'Min Time':<12} {'Max Time':<12} {'Tokens/s':<12} {'Speedup':<10}\")\n",
    "print(f\"{'-'*75}\")\n",
    "\n",
    "baseline_time = performance_results[\"Phase 1\"][\"avg_time\"]\n",
    "\n",
    "for phase, metrics in performance_results.items():\n",
    "    speedup = baseline_time / metrics[\"avg_time\"]\n",
    "    print(f\"{phase:<10} {metrics['avg_time']:<12.2f} {metrics['min_time']:<12.2f} {metrics['max_time']:<12.2f} {metrics['tokens_per_second']:<12.1f} {speedup:<10.2f}x\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(f\"\\nğŸ’¾ Memory Usage Analysis:\")\n",
    "if tpu_memory_manager:\n",
    "    final_memory_stats = tpu_memory_manager.get_memory_stats()\n",
    "    print(f\"   Current Memory Usage: {final_memory_stats.get('allocated_gb', 0):.2f} GB\")\n",
    "    print(f\"   Peak Memory Usage: {final_memory_stats.get('peak_gb', 0):.2f} GB\")\n",
    "    print(f\"   Memory Efficiency: {final_memory_stats.get('efficiency_percent', 0):.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Summary:\")\n",
    "best_phase = min(performance_results.keys(), key=lambda x: performance_results[x][\"avg_time\"])\n",
    "best_speedup = baseline_time / performance_results[best_phase][\"avg_time\"]\n",
    "print(f\"   ğŸ† Best Performance: {best_phase} ({best_speedup:.2f}x speedup)\")\n",
    "print(f\"   âš¡ Max Throughput: {max(perf['tokens_per_second'] for perf in performance_results.values()):.1f} tokens/second\")\n",
    "print(f\"   ğŸ® Recommended: Phase 2 for balanced performance, Phase 3 for batch workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8e36a",
   "metadata": {},
   "source": [
    "## 7. Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cf3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management and cleanup\n",
    "print(f\"ğŸ§¹ TPU Memory Management and Cleanup\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "# Get final memory statistics\n",
    "if tpu_memory_manager:\n",
    "    print(f\"ğŸ“Š Final Memory Statistics:\")\n",
    "    memory_stats = tpu_memory_manager.get_memory_stats()\n",
    "    \n",
    "    for key, value in memory_stats.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"   {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Cleanup operations\n",
    "print(f\"\\nğŸ”„ Performing cleanup operations...\")\n",
    "\n",
    "# Clear model from memory\n",
    "del model\n",
    "print(f\"   âœ… Model cleared from memory\")\n",
    "\n",
    "# Clear TPU cache\n",
    "if tpu_memory_manager:\n",
    "    tpu_memory_manager.clear_cache()\n",
    "    print(f\"   âœ… TPU cache cleared\")\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "print(f\"   âœ… Garbage collection completed\")\n",
    "\n",
    "# Final XLA synchronization\n",
    "xm.mark_step()\n",
    "print(f\"   âœ… XLA synchronization completed\")\n",
    "\n",
    "# Cleanup Phase 3 resources if available\n",
    "if phase3_manager:\n",
    "    try:\n",
    "        phase3_manager.cleanup_advanced_pipeline()\n",
    "        print(f\"   âœ… Phase 3 resources cleaned up\")\n",
    "    except:\n",
    "        print(f\"   âš ï¸ Phase 3 cleanup not available\")\n",
    "\n",
    "print(f\"\\nâœ… Cleanup completed successfully!\")\n",
    "print(f\"\\nğŸ‰ TPU Inference Example Complete!\")\n",
    "print(f\"   ğŸ“š You've successfully demonstrated all three phases of PantheraML TPU inference\")\n",
    "print(f\"   ğŸš€ Ready to scale up to production workloads\")\n",
    "print(f\"   ğŸ’¡ Consider using Phase 2 for most applications, Phase 3 for large-scale batch inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d0fc1",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "### ğŸ¯ Key Takeaways:\n",
    "\n",
    "1. **Phase 1 (Basic)**: Provides robust error handling and basic TPU support\n",
    "2. **Phase 2 (Optimized)**: Adds XLA compilation and performance optimizations\n",
    "3. **Phase 3 (Advanced)**: Enables multi-pod inference with JAX integration\n",
    "\n",
    "### âš¡ Performance Tips:\n",
    "\n",
    "- Use `bfloat16` precision for optimal TPU performance\n",
    "- Pad inputs to consistent lengths for XLA efficiency\n",
    "- Batch multiple prompts together when possible\n",
    "- Enable KV cache for autoregressive generation\n",
    "- Use `xm.mark_step()` for proper synchronization\n",
    "\n",
    "### ğŸ—ï¸ Production Recommendations:\n",
    "\n",
    "- **Phase 2** for most production inference workloads\n",
    "- **Phase 3** for large-scale batch processing\n",
    "- Monitor memory usage with PantheraML's memory manager\n",
    "- Implement proper error handling and fallbacks\n",
    "- Use auto-scaling for variable workloads\n",
    "\n",
    "### ğŸ“š Additional Resources:\n",
    "\n",
    "- [PantheraML Documentation](https://github.com/PantheraML/docs)\n",
    "- [TPU Performance Guide](https://cloud.google.com/tpu/docs/performance-guide)\n",
    "- [XLA Compilation Best Practices](https://www.tensorflow.org/xla)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy TPU inferencing with PantheraML! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
