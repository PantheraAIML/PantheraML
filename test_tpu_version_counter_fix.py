#!/usr/bin/env python3
"""
Test TPU version_counter fix for inference operations
"""

import os
import sys
import torch

def test_tpu_inference_context():
    """Test that TPU inference context avoids version_counter errors"""
    print("üß™ Testing TPU inference context...")
    
    # Simulate TPU environment
    os.environ["UNSLOTH_DEVICE_TYPE"] = "tpu"
    
    # Mock the device type
    class MockPantheraML:
        DEVICE_TYPE = "tpu"
    
    sys.modules['pantheraml'] = MockPantheraML()
    
    try:
        # Import our utility functions
        sys.path.insert(0, "/Users/aayanmishra/unsloth")
        from pantheraml.models._utils import get_inference_context, tpu_compatible_inference_mode
        
        print(f"   Device type: {MockPantheraML.DEVICE_TYPE}")
        
        # Test 1: get_inference_context should return torch.no_grad() for TPU
        context = get_inference_context()
        print(f"   ‚úÖ TPU inference context type: {type(context).__name__}")
        
        # Test 2: Test that we can use the context without errors
        with context:
            # Create a simple tensor operation
            x = torch.tensor([1.0, 2.0, 3.0])
            y = x * 2
            print(f"   ‚úÖ Tensor operation in TPU context successful: {y}")
        
        # Test 3: Test the decorator
        @tpu_compatible_inference_mode
        def simple_inference_function():
            x = torch.tensor([4.0, 5.0, 6.0])
            return x + 1
        
        result = simple_inference_function()
        print(f"   ‚úÖ Decorated function result: {result}")
        
        # Test 4: Test with CUDA device type
        MockPantheraML.DEVICE_TYPE = "cuda"
        context_cuda = get_inference_context()
        print(f"   ‚úÖ CUDA inference context type: {type(context_cuda).__name__}")
        
        print("‚úÖ All TPU inference context tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå TPU inference context test failed: {e}")
        return False
    finally:
        # Clean up
        if 'pantheraml' in sys.modules:
            del sys.modules['pantheraml']

def test_version_counter_simulation():
    """Test simulation of version_counter scenarios"""
    print("üß™ Testing version_counter error scenarios...")
    
    try:
        # Test that torch.no_grad() works for tensor operations
        with torch.no_grad():
            x = torch.tensor([1.0, 2.0, 3.0])
            # Simulate operations that might cause version_counter issues
            x = x.clone()  # This can sometimes cause version issues in inference_mode
            x = x.detach()
            result = x + 1
            print(f"   ‚úÖ torch.no_grad() operations successful: {result}")
        
        # Test inference_mode for comparison (should work on CPU)
        with torch.inference_mode():
            x = torch.tensor([1.0, 2.0, 3.0])
            result = x + 1  # Simple operations should work
            print(f"   ‚úÖ torch.inference_mode() operations successful: {result}")
        
        print("‚úÖ Version counter simulation tests passed!")
        return True
        
    except Exception as e:
        print(f"‚ùå Version counter simulation failed: {e}")
        return False

if __name__ == "__main__":
    print("üîß Testing TPU version_counter fixes...")
    print("=" * 60)
    
    success = True
    
    success &= test_tpu_inference_context()
    print()
    
    success &= test_version_counter_simulation()
    print()
    
    if success:
        print("üéâ All TPU version_counter fixes verified!")
        print("‚úÖ TPU inference uses torch.no_grad() instead of torch.inference_mode()")
        print("‚úÖ Other devices continue to use torch.inference_mode() for performance")
        print("‚úÖ Version counter errors should be resolved")
    else:
        print("‚ùå Some TPU version_counter issues remain")
        sys.exit(1)
